{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFLPseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'22/11/2021 14:16:52'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%d/%m/%Y %H:%M:%S') # last edited on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "# please fill out manually\n",
    "\n",
    "analysis_dir = '/media/data3/marco/AFLPseq/github_test_cov2/'\n",
    "filtered_reads_dir = '/media/data1/Nanopore_raw/20210630_Senecio_Philipp/20210630_Senecio/Senecio/primer_trimming/barcode0X_q7_50to1000bp/' # dir need to end with '/'\n",
    "barcodes = ['bc01', 'bc02', 'bc03', 'bc04','bc05', 'bc06', 'bc07', 'bc08', 'bc09'] # need to be given as a python list\n",
    "within_samples_clustering_ct = '0.75' # needs to be lower than the among-samples clustering threshold\n",
    "in_between_samples_clustering_ct = '0.90' # needs to be higher than the within-samples clustering threshold\n",
    "minimum_depth = 2\n",
    "minimum_sample_per_locus = 2 # needs to be at least 2\n",
    "threads = '10' # needs to be given in ''\n",
    "\n",
    "####################################################\n",
    "\n",
    "print('Analysis will be done in:', analysis_dir)\n",
    "print('Filtered reads will be taken from:', filtered_reads_dir)\n",
    "print('There are', len(barcodes), 'samples. Barcodes are:', barcodes)\n",
    "print('Clustering within-samples will have a clustering threshold of', within_samples_clustering_ct)\n",
    "print('Clustering among-samples will have a clustering threshold of', in_between_samples_clustering_ct)\n",
    "print('Depth will be at a minimum of', minimum_depth)\n",
    "print('There need to be at least', minimum_sample_per_locus, 'samples to accept a locus')\n",
    "print('Clustering will use', threads, 'thread(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from natsort import natsorted, ns\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from itertools import islice\n",
    "import shutil\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import time\n",
    "import ntpath\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "def path_leaf(path):\n",
    "    '''Returns only the filename as a string if a path is given'''\n",
    "    head, tail = ntpath.split(path)\n",
    "    return tail or ntpath.basename(head)\n",
    "\n",
    "def get_infos_from_cluster_file(cluster_file_path):\n",
    "    '''Extracts barcode number and read id from vsearch cluster file'''\n",
    "    infos = []\n",
    "    with open(cluster_file_path) as cf:\n",
    "        for line in cf:\n",
    "            if line.startswith('>'):\n",
    "                entry_name, entry_id, entry_nseq = line.strip().split('=')\n",
    "                entry_name = entry_name[1:].replace('_centroid', '')\n",
    "                entry_id = entry_id.split(';')[0]                \n",
    "                infos.append([entry_name, entry_id])    \n",
    "        return infos\n",
    "\n",
    "def write_infos_to_file(infos, info_file, mode='x'):\n",
    "    '''Writes barcode number and read id from vsearch cluster file, extracted by get_infos_from_cluster_file(), to output file'''\n",
    "    with open(info_file, mode) as f:\n",
    "        f.write('barcode\\tread_id\\n')\n",
    "        for entry in infos:\n",
    "            f.write(f'{entry[0]}\\t{entry[1]}\\n')\n",
    "            \n",
    "def take_four(it):\n",
    "    '''While there are elements in input, take four elements'''\n",
    "    fastq_entries = []\n",
    "    \n",
    "    _it = iter(it)\n",
    "    while True:\n",
    "        try:\n",
    "            yield next(_it), next(_it), next(_it), next(_it)\n",
    "        except StopIteration:\n",
    "            # no more elements in the iterator\n",
    "            return\n",
    "\n",
    "def consout_to_dict(infile):\n",
    "    '''Parses vsearch consout fasta output and returns it as a dictionary'''\n",
    "    with open(infile) as cons_file:\n",
    "        test_dict = {}\n",
    "        \n",
    "        cons_file = cons_file.read().split('>')                                    # splits the fasta file by '>'\n",
    "        while '' in cons_file:                                                     # removes empty list elements\n",
    "            cons_file.remove('')\n",
    "        cons_file = ['>' + i for i in cons_file]                                   # restores fasta header by readding '>'\n",
    "        \n",
    "        for i in cons_file:                                                        # loop through consout fasta entries\n",
    "            if i.startswith('>'):\n",
    "                centroid_read_id = i[i.find('_centroid=')+10:i.find(';seqs=')]     # extracts centroid read id\n",
    "                sequence = i[i.find('\\n')+1:]                                      # extracts the DNA sequence\n",
    "                test_dict[centroid_read_id] = sequence                             # adds fasta entry to dictionary\n",
    "                                                                                   # dictionary key = centroid read id\n",
    "        return test_dict                                                           # dictionary value = DNA sequence\n",
    "\n",
    "    \n",
    "# create directory for full analysis\n",
    "\n",
    "if os.path.exists(analysis_dir):                                                             # check if directory already exists\n",
    "    userinput = input('Chosen directory already exists. Do you wish to overwrite? (y/n)')    # directory exists, ask user to overwrite\n",
    "    print('\\n')\n",
    "    if userinput == 'n':\n",
    "        print('Directory was not overwritten')\n",
    "    if userinput == 'y':                                                                     # user chose yes\n",
    "        shutil.rmtree(analysis_dir)                                                          # remove directory with content\n",
    "        if not os.path.exists(analysis_dir):                                                 # if directory now does not exist:\n",
    "            os.makedirs(analysis_dir)                                                        # make the directory\n",
    "            print(analysis_dir, 'was created')\n",
    "else:\n",
    "    if not os.path.exists(analysis_dir):                                                     # if directory does not exist:\n",
    "        os.makedirs(analysis_dir)                                                            # make the directory\n",
    "        print(analysis_dir, 'was created')\n",
    "\n",
    "        \n",
    "# create directory for within-sample-clustering\n",
    "\n",
    "within_sampe_clustering_dir = analysis_dir + '/within_sample_clustering'    # name and path within-sample-clustering directory\n",
    "if not os.path.exists(within_sampe_clustering_dir):\n",
    "    os.makedirs(within_sampe_clustering_dir)                               # make the directory\n",
    "    print(within_sampe_clustering_dir, 'was created')\n",
    "else:\n",
    "    print(within_sampe_clustering_dir, 'seems to already exist')\n",
    "\n",
    "\n",
    "# create paths for all defined barcodes\n",
    "\n",
    "for i in range(len(barcodes)):\n",
    "    wsc_sample_dir = within_sampe_clustering_dir + '/' + barcodes[i]\n",
    "    os.makedirs(wsc_sample_dir)\n",
    "    print(wsc_sample_dir, 'was created')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# vsearch within samples clustering\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "samplename_args = filtered_reads_dir + '/*.fastq'\n",
    "samplename = glob.glob(samplename_args)             # list all fastqs in input read directory\n",
    "\n",
    "\n",
    "for i in range(len(barcodes)):                                                            # loop through each sample/barcode\n",
    "    \n",
    "    wsc_sample_dir = within_sampe_clustering_dir + '/' + barcodes[i]                      # define working directory for each sample/barcode\n",
    "    \n",
    "    msaout = wsc_sample_dir + '/msaout_' + barcodes[i]                                    # define the vsearch msaout path and name\n",
    "    consout = wsc_sample_dir + '/consout_' + barcodes[i]                                  # define the vsearch consout path and name\n",
    "    clusters_dir = wsc_sample_dir + '/clusters'                                           # define the vsearch clusters path and directory name\n",
    "    \n",
    "    if not os.path.exists(clusters_dir):                                                  # if the clusters directory does not yet exist\n",
    "        os.makedirs(clusters_dir)                                                         # create it\n",
    "    \n",
    "    clusters = clusters_dir + '/' + barcodes[i] + '_cluster_'                             # define the vsearch clusters names\n",
    "    stdout_path = wsc_sample_dir + '/vsearch_stdout'                                      # define the vsearch stdout\n",
    "    stderr_path = wsc_sample_dir + '/vsearch_stderr'                                      # define the vsearch stderr\n",
    "    \n",
    "    for j in samplename:\n",
    "        if barcodes[i] in path_leaf(j):\n",
    "            vsearch_input = j\n",
    "\n",
    "    #vsearch_input = filtered_reads_dir + barcodes[i] + '_q7_200to1000bp.fastq'            # define the path to the correct sample.fastq\n",
    "    \n",
    "    vsearch_wsc_args = ['vsearch', \n",
    "                        '--cluster_fast', vsearch_input, \n",
    "                        '--id', within_samples_clustering_ct, \n",
    "                        '--msaout', msaout, \n",
    "                        '--consout', consout, \n",
    "                        '--clusters', clusters, \n",
    "                        '--threads', threads]                                             # list of the vsearch command line input formatted for the subprocess package\n",
    "    \n",
    "    if os.path.exists(stdout_path):                                                       # remove vsearch stdout and stderr if it already exists from previous runs\n",
    "        os.remove(stdout_path)\n",
    "    if os.path.exists(stderr_path):\n",
    "        os.remove(stderr_path)\n",
    "    \n",
    "    loopcounter = loopcounter + 1  \n",
    "    print('Sample', loopcounter, 'of', len(barcodes), ': Clustering', path_leaf(vsearch_input), '(', barcodes[i], ') at vsearch --id', within_samples_clustering_ct, '...', end='\\r')\n",
    "   \n",
    "    with open(stdout_path, 'x') as stdout_file:                                                                   # create and open files to write stderr and stdout to\n",
    "        with open(stderr_path, 'x') as stderr_file:\n",
    "            run_vsearch_wsc = subprocess.run(vsearch_wsc_args, stdout = stdout_file, stderr = stderr_file)        # run vsearch\n",
    "\n",
    "            if run_vsearch_wsc.returncode == 1:                                                                   # print a warning message if vsearch process was unsuccessful\n",
    "                print('Warning. Something went wrong concerning the within-sample-clustering of', barcodes[i])\n",
    "                print(run_vsearch_wsc, '\\n')\n",
    "                break\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# count reads in every cluster = clustercount\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in range(len(barcodes)):                                                       # loop through every sample\n",
    "    loopcounter = loopcounter + 1  \n",
    "    \n",
    "    wsc_sample_dir = within_sampe_clustering_dir + '/' + barcodes[i]                 # define working directory for the current sample/barcode\n",
    "    \n",
    "    clustercount_dir = wsc_sample_dir + '/clustercount_' + barcodes[i] + '.csv'      # define path and name for the clustercount.csv\n",
    "\n",
    "    if os.path.exists(clustercount_dir):                                             # if the clustercount.csv already exists (from a previous run)\n",
    "        os.remove(clustercount_dir)                                                  # remove it\n",
    "    if not os.path.exists(clustercount_dir):                                         # if it does not exist yet\n",
    "        with open(clustercount_dir, 'x') as f:                                       # create and open the clustercount.csv\n",
    "            f.write('cluster\\tcount\\n')                                              # write a header with 2 columns: cluster(name) and (read)count\n",
    "\n",
    "    clusters_in_dir = wsc_sample_dir + '/clusters' + '/*'                            # define the vsearch clusters directory\n",
    "    list_clusters = glob.glob(clusters_in_dir)                                       # list those cluster fasta files\n",
    "    \n",
    "    # inform about the current progress of the script\n",
    "    print('Sample', loopcounter, 'out of', len(barcodes), ': Counting reads per cluster of', barcodes[i], '(', len(list_clusters), 'clusters )', end='\\r')\n",
    "    \n",
    "    for j in list_clusters:                                # loop through each vsearch clusters file\n",
    "        with open(j) as f:                                 # open a cluster fasta file from the list\n",
    "            n = 0                                          # start a counter at zero\n",
    "            clustercount = []                              # placeholder variable for the (read)count of the current cluster\n",
    "            for line in f:                                 # loop through the cluster fasta file line per line\n",
    "                if line[0] == '>':                         # if it is a header line starting with '>'\n",
    "                    n = n + 1                              # count + 1\n",
    "            clustercount.append(n)                         # add the final (read)count number to the placeholder\n",
    "            \n",
    "            with open(clustercount_dir, 'a') as f:         # open the clustercount.csv file\n",
    "                f.write(path_leaf(j))                      # write in the 'cluster' column the clustername\n",
    "                f.write('\\t')\n",
    "                f.write(str(clustercount[0]))              # write in the 'count' column the (read)count number\n",
    "                f.write('\\n')\n",
    "    \n",
    "    clustercount_df = pd.read_csv(clustercount_dir, sep='\\t')                        # open the finished clustercount.csv in pandas\n",
    "    clustercount_hist = clustercount_dir.replace('.csv', '_hist.csv')                # name and path the clustercount histogram output\n",
    "    clustercount_df.groupby('count').count().to_csv(clustercount_hist, sep=\"\\t\")     # write a histogram file counting how many clusters have how many reads\n",
    "\n",
    "print('\\n')    \n",
    "\n",
    "\n",
    "# prepare a list of just the normal clustercount.csv without the _hist versions\n",
    "\n",
    "path_to_clustercount_csvs = within_sampe_clustering_dir + '/*/clustercount*.csv'     # gather all clustercount files (also lists the hist versions)\n",
    "list_of_clustercount_files = glob.glob(path_to_clustercount_csvs)\n",
    "\n",
    "remove_hist_files = []\n",
    "\n",
    "for i in list_of_clustercount_files:\n",
    "    if 'hist' in path_leaf(i):\n",
    "        remove_hist_files.append(i)\n",
    "\n",
    "list_of_clustercount_files = [x for x in list_of_clustercount_files if x not in remove_hist_files]\n",
    "\n",
    "        \n",
    "# loop through all (non-histogram (removed above)) clustercount files for statistics to print\n",
    "\n",
    "for i in list_of_clustercount_files:\n",
    "    print(i)\n",
    "    df = pd.read_csv(i, sep='\\t')\n",
    "    print('Total clusters:\\t\\t\\t\\t', len(df))\n",
    "    print('Total reads:\\t\\t\\t\\t', df['count'].sum())\n",
    "    print('Mean reads in cluster:\\t\\t\\t', round(df['count'].mean(), 2), '+-', round(df['count'].std(), 2))\n",
    "    print('Median reads in cluster:\\t\\t', round(df['count'].median(), 2))\n",
    "    print('Singleton clusters:\\t\\t\\t', df['count'].isin([1]).sum(), \n",
    "          '(', (round(df['count'].isin([1]).sum() / len(df), 4) * 100), '% )')\n",
    "    print('Clusters with', minimum_depth, 'or more reads:\\t\\t', df[df['count'] >= minimum_depth].count()[0],\n",
    "          '(', (round(df[df['count'] >= minimum_depth].count()[0] / len(df), 4) * 100), '% )')\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "# add an identifer to the consensus fasta sequences in consout vsearch output\n",
    "\n",
    "list_of_consout = []\n",
    "for i in range(len(barcodes)):\n",
    "    wsc_sample_dir = within_sampe_clustering_dir + '/' + barcodes[i]\n",
    "    consout = wsc_sample_dir + '/consout_' + barcodes[i]\n",
    "    list_of_consout.append(consout)\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in list_of_consout:                                                 # loop through the consout files\n",
    "    loopcounter = loopcounter + 1\n",
    "    print('Sample', loopcounter, 'out of', len(list_of_consout), ': Writing the barcode in the consout fasta headers for', i, end='\\r')\n",
    "    for j in barcodes:                                                    # loop through barcodes\n",
    "        if j in path_leaf(i):                                             # if a barcode matches the barcode in the consout file name:\n",
    "            with open(i) as f:                                            # open the respective consout file\n",
    "                outfile_path = i + '_with_barcode.fasta'                  # name the output with barcode consout file after the respective consout\n",
    "                if os.path.exists(outfile_path):\n",
    "                    os.remove(outfile_path)                               # remove any output from previous runs\n",
    "                if not os.path.exists(outfile_path):\n",
    "                    with open(outfile_path, 'x') as outfile:              # create and open the consout_filtered.fasta file\n",
    "                        for line in f:\n",
    "                            if line.startswith('>'):\n",
    "                                replacer = '>' + j + '_'\n",
    "                                line = line.replace('>', replacer)        # add the respective barcode to the fasta header behind the '>'\n",
    "                            outfile.write(line)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# filter out low-depth clusters in the vsearch consout file   \n",
    "\n",
    "consout_dirs = within_sampe_clustering_dir + '/*/consout*_with_barcode.fasta'\n",
    "list_of_consout = glob.glob(consout_dirs)\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in list_of_consout:\n",
    "    \n",
    "    loopcounter = loopcounter + 1\n",
    "    for bc in barcodes:\n",
    "        if bc in path_leaf(i):\n",
    "            current_sample = bc\n",
    "    print('Sample', loopcounter, 'out of', len(list_of_consout), ': Filtering sample', current_sample, 'for clusters >=', minimum_depth, 'reads...')\n",
    "    \n",
    "    with open(i) as f:\n",
    "        # parse the fasta\n",
    "        content = f.read()                                    # loads fasta in readable form \n",
    "        content_split = content.split('>')                    # create a list of the fasta, splitting entries by '>'\n",
    "        while '' in content_split:\n",
    "            content_split.remove('')                          # remove emtpy entries in list\n",
    "        content_split = [\">\" + j for j in content_split]      # restore the fasta format by readding '>' at the beginning of the header\n",
    "        \n",
    "        # write the filtered consout file\n",
    "        outfile_path = i.replace('_with_barcode.fasta', '_filtered.fasta')              # name the output 'consout*_filtered.fasta'\n",
    "        if os.path.exists(outfile_path):\n",
    "            os.remove(outfile_path)                                                     # remove any output from previous runs \n",
    "        if not os.path.exists(outfile_path):\n",
    "            with open(outfile_path, 'x') as outfile:                                    # create and open 'consout*_filtered.fasta' file\n",
    "                for j in content_split:\n",
    "                    if int(j[j.find(';seqs=')+6 : j.find('\\n')]) >= minimum_depth:      # if seqs= is higher than or exactly the defined minimum depth:\n",
    "                        outfile.write(j)                                                # write to 'consout*_filtered.fasta'\n",
    "\n",
    "            # how many clusters are left?\n",
    "            with open(outfile_path) as outfile:      # reopen the newly written consout_*_filtered.fasta\n",
    "                n = 0\n",
    "                conscount = []\n",
    "                for line in outfile:\n",
    "                    if line[0] == '>':\n",
    "                        n = n + 1                    # count + 1 if there is a '>' = one fasta line\n",
    "                conscount.append(n)                  # append the number to the conscount variable\n",
    "                \n",
    "                # print details to within-sample clustering depth filtering\n",
    "                print('Filtered out', (len(content_split) - conscount[0]), 'low-depth clusters', 'out of', len(content_split), '.', conscount[0], 'clusters remain for', current_sample)              \n",
    "    \n",
    "    os.remove(i) # remove the '_with_barcode.fasta' file as it is now redundant\n",
    "                            \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# Move clusters with 'depth' >= 'minimum_depth' to 'depth_filtered_clusters_dir'\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for bc in barcodes:\n",
    "    unfiltered_clusters_args = within_sampe_clustering_dir + '/' + bc + '/clusters/*'\n",
    "    unfiltered_clusters = glob.glob(unfiltered_clusters_args)\n",
    "    depth_filtered_clusters_dir = within_sampe_clustering_dir + '/' + bc + '/depth_filtered_clusters'\n",
    "    \n",
    "    loopcounter = loopcounter + 1\n",
    "    print(loopcounter, 'out of', len(barcodes), ': Moving', bc, 'clusters with >=', minimum_depth, 'depth to', depth_filtered_clusters_dir, end='\\r')\n",
    "    \n",
    "    # create directory for depth-filtered clusters\n",
    "    \n",
    "    if os.path.exists(depth_filtered_clusters_dir):\n",
    "        shutil.rmtree(depth_filtered_clusters_dir)\n",
    "    if not os.path.exists(depth_filtered_clusters_dir):\n",
    "        os.makedirs(depth_filtered_clusters_dir)\n",
    "    \n",
    "    \n",
    "    # Move clusters with 'depth' >= 'minimum_depth' to 'depth_filtered_clusters_dir'\n",
    "\n",
    "    for i in unfiltered_clusters:\n",
    "        with open(i) as f:\n",
    "            depth = 0\n",
    "            for line in f.read():\n",
    "                if line.startswith('>'):\n",
    "                    depth = depth + 1\n",
    "            if depth >= minimum_depth:\n",
    "                shutil.move(i, depth_filtered_clusters_dir) \n",
    "\n",
    "print('\\n')\n",
    "\n",
    "          \n",
    "# map reads to consensus sequence\n",
    "          \n",
    "list_of_consout_args = within_sampe_clustering_dir + '/*/consout*filtered.fasta'\n",
    "list_of_consout = glob.glob(list_of_consout_args)\n",
    "\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in list_of_consout:\n",
    "    # select the corresponding /within_samples_clustering/*/depth_filtered_clusters directory\n",
    "    \n",
    "    for bc in barcodes:\n",
    "        if bc in path_leaf(i):\n",
    "            clusters_dir_args = within_sampe_clustering_dir + '/' + bc + '/depth_filtered_clusters/*'\n",
    "            clusters_dir = glob.glob(clusters_dir_args)\n",
    "            \n",
    "            consout_ref_files = within_sampe_clustering_dir + '/' + bc + '/clusters_reads_to_consout_mapping'\n",
    "            \n",
    "            if os.path.exists(consout_ref_files):\n",
    "                shutil.rmtree(consout_ref_files)\n",
    "            if not os.path.exists(consout_ref_files):\n",
    "                os.makedirs(consout_ref_files)\n",
    "    \n",
    "            \n",
    "    # select centroid ids and search for them in the clusters directory to identify corresponding reads\n",
    "            \n",
    "    with open(i) as f:\n",
    "        loopcounter = loopcounter + 1\n",
    "\n",
    "        # parse consout.fasta\n",
    "        content = f.read().split('>')\n",
    "        while '' in content:\n",
    "            content.remove('')\n",
    "        content = ['>' + j for j in content]\n",
    "        \n",
    "        loopcounter2 = 0\n",
    "        \n",
    "        for k in content:\n",
    "            #print(k)\n",
    "            \n",
    "            loopcounter2 = loopcounter2 + 1\n",
    "            print(loopcounter2, 'out of', len(content), \n",
    "                  '- Currently mapping reads to their correspondong consout.fasta entry:', path_leaf(i), \n",
    "                  '(', loopcounter, 'out of', len(list_of_consout), 'consouts )', end='\\r')\n",
    "            \n",
    "            if k.startswith('>'):\n",
    "                header = k[k.find('_centroid=')+10 : k.find(';seqs=')]         # extract centroid read id from consout fasta entry\n",
    "                #print(header)\n",
    "                \n",
    "                consout_ref = consout_ref_files + '/' + header                 # name reference file\n",
    "                #print(consout_ref)\n",
    "                \n",
    "                with open(consout_ref, 'x') as consout_ref_f:\n",
    "                    consout_ref_f.write(k)                                     # write reference to file\n",
    "                \n",
    "                \n",
    "                # find corresponding 'within_sample_clustering/*/clusters/cluster_*'\n",
    "                \n",
    "                for j in clusters_dir:\n",
    "                    with open(j) as cluster:\n",
    "                        if header in cluster.read():\n",
    "                            #print('Found header in', j)\n",
    "                    \n",
    "                            \n",
    "                            # mapping to reference\n",
    "                            \n",
    "                            stdout_path = consout_ref_files + '/' + path_leaf(j) + '.sam'        # mapping output .sam file\n",
    "                            if os.path.exists(stdout_path):\n",
    "                                os.remove(stdout_path)\n",
    "                            with open(stdout_path, 'x') as stdout_outfile:\n",
    "                                mapping = subprocess.run(['minimap2', \n",
    "                                                          '-ax', 'map-ont', \n",
    "                                                          '-t', threads, \n",
    "                                                          consout_ref,                 # reference path\n",
    "                                                          j],                          # input fastq path\n",
    "                                                          stdout = stdout_outfile      # minimap2 .sam output\n",
    "                                                        )\n",
    "                            if mapping.returncode == 1:\n",
    "                                print('Error. Subprocess for minimap2 mapping returncode = 1. Please check your output.')\n",
    "                                break\n",
    "         \n",
    "            else:\n",
    "                print('Error.', k, 'is not in correct FASTA format.')\n",
    "                break\n",
    "\n",
    "\n",
    "# log mapping data (clusters with unmapped reads)\n",
    "                \n",
    "unmapped_reads_log_dir_wsc = within_sampe_clustering_dir + '/unmapped_reads_log.csv'\n",
    "\n",
    "if os.path.exists(unmapped_reads_log_dir_wsc):                                                          # check if missing reads in the mapping file exists\n",
    "    os.remove(unmapped_reads_log_dir_wsc)                                                               # remove it\n",
    "if not os.path.exists(unmapped_reads_log_dir_wsc):                                                      # if file does not exist\n",
    "    with open(unmapped_reads_log_dir_wsc, 'x') as f:                                                    # make it\n",
    "        f.write('barcode\\tcluster\\tunmapped reads\\ttotal reads in cluster\\tPercentage unmapped reads\\n')     # and write the header\n",
    "    print('Unmapped reads are logged to:', unmapped_reads_log_dir_wsc)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "list_of_all_sam_args = within_sampe_clustering_dir + '/*/clusters_reads_to_consout_mapping/*.sam'\n",
    "list_of_all_sam = glob.glob(list_of_all_sam_args)                                                      # list the sam files made from minimap2 mapping above\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in list_of_all_sam:\n",
    "    loopcounter = loopcounter + 1\n",
    "    print('Now counting unmapped reads in all .sam files...', loopcounter, 'out of', len(list_of_all_sam), end='\\r')\n",
    "    \n",
    "    for bc in barcodes:\n",
    "        if bc in i:\n",
    "            barcode = bc\n",
    "    \n",
    "    with open(i) as f:\n",
    "        unmapped_reads_count = 0\n",
    "        total_reads = []\n",
    "        for line in f:\n",
    "            if not line.startswith('@'):\n",
    "                line = line.split('\\t')\n",
    "                if line[0] not in total_reads:\n",
    "                    total_reads.append(line[0])\n",
    "                if line[2] == '*':\n",
    "                    unmapped_reads_count = unmapped_reads_count + 1\n",
    "        #print(unmapped_reads_count, 'out of', len(total_reads), 'reads are unmapped')\n",
    "   \n",
    "    with open(unmapped_reads_log_dir_wsc, 'a') as unmapped_reads_f:\n",
    "        unmapped_reads_f.write(barcode)\n",
    "        unmapped_reads_f.write('\\t')\n",
    "        unmapped_reads_f.write(path_leaf(i))\n",
    "        unmapped_reads_f.write('\\t')\n",
    "        unmapped_reads_f.write(str(unmapped_reads_count))\n",
    "        unmapped_reads_f.write('\\t')\n",
    "        unmapped_reads_f.write(str(len(total_reads)))\n",
    "        unmapped_reads_f.write('\\t')\n",
    "        percentage_unmapped_reads = round( ( (unmapped_reads_count / len(total_reads) ) * 100), 2)\n",
    "        unmapped_reads_f.write(str(percentage_unmapped_reads))\n",
    "        unmapped_reads_f.write('\\n')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# open the unmapped_reads_log.csv for statistics\n",
    "\n",
    "df = pd.read_csv(unmapped_reads_log_dir_wsc, sep='\\t')\n",
    "df['mapped reads'] = df['total reads in cluster'] - df['unmapped reads'] # adding a column with mapped reads\n",
    "\n",
    "print('depth-filtered clusters:\\t\\t\\t', len(df))\n",
    "print('clusters with unmapped reads:\\t\\t\\t', len(df[df['unmapped reads'] > 0]),\n",
    "      '(', round(len(df[df['unmapped reads'] > 0]) / len(df), 4) * 100, '% of depth-filtered clusters)',\n",
    "      '( mean sample:', len(df[df['unmapped reads'] > 0]) / len(barcodes), ')')\n",
    "print('clusters with less than', minimum_depth, 'unmapped reads:\\t', len(df[df['unmapped reads'] < minimum_depth]),\n",
    "      '(', round(len(df[df['unmapped reads'] < minimum_depth]) / len(df), 4) * 100, '% of depth-filtered clusters)')\n",
    "print('clusters with', minimum_depth, 'or more unmapped reads:\\t', len(df[df['unmapped reads'] >= minimum_depth]),\n",
    "      '(', round((len(df[df['unmapped reads'] >= minimum_depth]) / len(df)) * 100, 2), '% of depth-filtered clusters)')\n",
    "print('clusters with', minimum_depth, 'or more mapped reads:\\t\\t', len(df[df['mapped reads'] >= minimum_depth]),\n",
    "      '(', round(len(df[df['mapped reads'] >= minimum_depth]) / len(df), 4) * 100, '% of depth-filtered clusters )')\n",
    "print('\\n')\n",
    "\n",
    "# create a pandas dataframe with clustering and mapping data for each sample for the user to assess an optimal clustering threshold for within-samples-clustering\n",
    "\n",
    "wsc_data = {}                       # empty placeholder dictionary\n",
    "wsc_df = pd.DataFrame(wsc_data)     # create the dataframe by using an empty dictionary\n",
    "wsc_df['samples'] = barcodes        # add samples as rows\n",
    "\n",
    "# add 'total clusters' and 'clusters with >= minimum_depth' as a pandas dataframe column\n",
    "total_clusters_column = {}       # placeholder dictionary to add entries to\n",
    "clusters_mindepth_column = {}\n",
    "\n",
    "for i in barcodes:                                                # loop through barcodes\n",
    "    for j in list_of_clustercount_files:                          # loop through the clustercount files\n",
    "        if i in path_leaf(j):                                     # if current barcode matches one of the clustercount files\n",
    "            df = pd.read_csv(j, sep='\\t')                         # load the clustercount file as a pandas dataframe\n",
    "            tmp_dict_1 = {len(df) : i}                            # combine barcode with its total cluster value\n",
    "            total_clusters_column.update(tmp_dict_1)              # add dictionary entry to placeholder dictionary\n",
    "            \n",
    "            tmp_df = df[df['count'] >= minimum_depth].count()[0]  # count \"clusters with >= minimum_depth reads\" for barcode i\n",
    "            tmp_dict_2 = {tmp_df : i}                             # combine barcode i with its \"clusters with >= minimum_depth reads\" value\n",
    "            clusters_mindepth_column.update(tmp_dict_2)           # add dictionary entry to placeholder dictionary\n",
    "\n",
    "# add 'total clusters' and 'clusters with >= minimum_depth' as a pandas dataframe column\n",
    "total_clusters_column = []       # placeholder dictionary to add entries to\n",
    "clusters_mindepth_column = []\n",
    "\n",
    "test_barcodes = ['bc09']\n",
    "\n",
    "for i in barcodes:                                                # loop through barcodes\n",
    "    for j in list_of_clustercount_files:                          # loop through the clustercount files\n",
    "        if i in path_leaf(j):                                     # if current barcode matches one of the clustercount files\n",
    "            df = pd.read_csv(j, sep='\\t')                         # load the clustercount file as a pandas dataframe\n",
    "            total_clusters_column.append(len(df))                 # count total clusters by length of the clustercount file\n",
    "            clusters_mindepth_column.append(df[df['count'] >= minimum_depth].count()[0])  # count \"clusters with >= minimum_depth reads\" for barcode i\n",
    "            \n",
    "# add mapping data to pandas dataframe as a column\n",
    "df = pd.read_csv(unmapped_reads_log_dir_wsc, sep='\\t')                      # load the 'unmapped_reads_log_dir_wsc' as 'df'\n",
    "df['mapped reads'] = df['total reads in cluster'] - df['unmapped reads']    # adding a column with mapped reads\n",
    "\n",
    "depth_filtered_clusters_column = []         # empty placeholder dictionaries to add entries to\n",
    "unmapped_reads_clusters_column = []\n",
    "less_than_min_depth_unmapped_column = []\n",
    "mincov_or_more_unmapped_column = []\n",
    "mincov_or_more_mapped_column = []\n",
    "\n",
    "for i in barcodes:\n",
    "    df_i = df[df['barcode'] == i]\n",
    "    depth_filtered_clusters_column.append(len(df_i))\n",
    "    unmapped_reads_clusters_column.append(len(df_i[df_i['unmapped reads'] > 0]))\n",
    "    less_than_min_depth_unmapped_column.append(len(df_i[df_i['unmapped reads'] < minimum_depth]))\n",
    "    mincov_or_more_unmapped_column.append(len(df_i[df_i['unmapped reads'] >= minimum_depth]))\n",
    "    mincov_or_more_mapped_column.append(len(df_i[df_i['mapped reads'] >= minimum_depth]))   \n",
    "\n",
    "    \n",
    "# add completed (placeholder) dictionaries as columns to pandas dataframe\n",
    "wsc_df['total clusters'] = total_clusters_column\n",
    "\n",
    "column_name_arg = 'clusters with >=' + str(minimum_depth) + ' reads'\n",
    "wsc_df[column_name_arg] = clusters_mindepth_column\n",
    "\n",
    "wsc_df['depth-filtered clusters'] = depth_filtered_clusters_column\n",
    "\n",
    "wsc_df['clusters with unmapped reads'] = unmapped_reads_clusters_column\n",
    "\n",
    "column_name_arg = 'clusters with <' + str(minimum_depth) + ' unmapped reads'\n",
    "wsc_df[column_name_arg] = less_than_min_depth_unmapped_column\n",
    "\n",
    "column_name_arg = 'clusters with >=' + str(minimum_depth) + ' unmapped reads'\n",
    "wsc_df[column_name_arg] = mincov_or_more_unmapped_column\n",
    "\n",
    "column_name_arg = 'clusters with >=' + str(minimum_depth) + ' mapped reads'\n",
    "wsc_df[column_name_arg] = mincov_or_more_mapped_column\n",
    "\n",
    "\n",
    "# save the pandas dataframe as a csv\n",
    "wsc_optimization_file = within_sampe_clustering_dir + '/clustering_mapping_statistics.csv'\n",
    "\n",
    "if os.path.exists(wsc_optimization_file):\n",
    "    os.remove(wsc_optimization_file)\n",
    "    \n",
    "wsc_df.to_csv(wsc_optimization_file, sep='\\t')\n",
    "print('An in-depth .csv file was placed here:', wsc_optimization_file)\n",
    "print('\\n')\n",
    "\n",
    "# remove consout consensus sequence entries with < 'minimum_depth' mapped reads\n",
    "\n",
    "df = pd.read_csv(unmapped_reads_log_dir_wsc, sep='\\t')                     # open unmapped reads csv in pandas\n",
    "df['mapped reads'] = df['total reads in cluster'] - df['unmapped reads']   # adding a column with mapped reads\n",
    "df_low_mapped_reads = df[df['mapped reads'] < minimum_depth]               # reducing the dataset to cluster with < 'minimum_depth' mapped reads\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for bc in barcodes:\n",
    "    loopcounter = loopcounter + 1\n",
    "    print('Sample', loopcounter, 'out of', len(barcodes), ': Filter out consensus sequences in', bc, 'consout with <', minimum_depth, 'mapped reads', end='\\r')\n",
    "    \n",
    "    df_bc_low_mapped_reads = df_low_mapped_reads[df_low_mapped_reads['barcode'] == bc]    # get data for current barcode\n",
    "    list_low_mapped_reads = df_bc_low_mapped_reads['cluster'].tolist()                    # list cluster with < 'minimum_depth' mapped reads for current barcode\n",
    "    \n",
    "    \n",
    "    # list centroid read ids from sam files (will be used to filter out consensus sequences in vsearch consout with < 'minimum_depth' mapped reads)\n",
    "    consout_rm_list = []          # placeholder list for appending centroid read ids of clusters with < 'minimum_depth' mapped reads to\n",
    "    \n",
    "    for i in list_low_mapped_reads:\n",
    "        path_to_sam = within_sampe_clustering_dir + '/' + bc + '/clusters_reads_to_consout_mapping/' + i       # get full path to sam file\n",
    "        with open(path_to_sam) as f:\n",
    "            content = f.read()\n",
    "            rm_centroid_read_id = content[content.find('_centroid=')+10 : content.find(';seqs=')]              # get centroid read id whose entry in consout is to be removed\n",
    "            consout_rm_list.append(rm_centroid_read_id)\n",
    "     \n",
    "    \n",
    "    # parse vsearch consout.fasta\n",
    "    consout_path = within_sampe_clustering_dir + '/' + bc + '/consout_' + bc + '_filtered.fasta'\n",
    "    with open(consout_path) as consout:\n",
    "        consout = consout.read().split('>')\n",
    "        while '' in consout:\n",
    "            consout.remove('')\n",
    "        consout = ['>' + l for l in consout]\n",
    "    \n",
    "    \n",
    "    # remove consensus sequences with < 'minimum_depth' mapped reads from consout.fasta\n",
    "    to_remove = []                                  # placeholder list with consensus sequences to remove\n",
    "    \n",
    "    for consensus in consout:\n",
    "        for read_id in consout_rm_list:\n",
    "            if read_id in consensus:\n",
    "                    to_remove.append(consensus)     # append consensus sequence to remove to list above\n",
    "\n",
    "    for consensus in to_remove:\n",
    "        consout.remove(consensus)                   # safely remove consensus sequence from consout.fasta\n",
    "        \n",
    "        \n",
    "    # overwrite the consout.fasta with the filtered consout consensus sequences\n",
    "    if os.path.exists(consout_path):\n",
    "        os.remove(consout_path)\n",
    "    if not os.path.exists(consout_path):\n",
    "        with open(consout_path, 'x') as f:\n",
    "            for entry in consout:\n",
    "                f.write(entry)\n",
    "                \n",
    "print('\\n')\n",
    "\n",
    "# remove unmapped reads from clusters.fasta in /depth_filtered_clusters/*\n",
    "\n",
    "# load df to identify clusters with unmapped reads\n",
    "df = pd.read_csv(unmapped_reads_log_dir_wsc, sep='\\t')\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for bc in barcodes:\n",
    "    loopcounter = loopcounter + 1\n",
    "    \n",
    "    # create (temporary) directories for clusters.fasta with only mapped reads\n",
    "    mapped_reads_only_dir = within_sampe_clustering_dir + '/' + bc + '/mapped_reads_only_clusters'\n",
    "    if os.path.exists(mapped_reads_only_dir):\n",
    "        shutil.rmtree(mapped_reads_only_dir)\n",
    "    if not os.path.exists(mapped_reads_only_dir):\n",
    "        os.makedirs(mapped_reads_only_dir)\n",
    "    \n",
    "    \n",
    "    # list the sam files with unmapped reads\n",
    "    df_bc = df[df['barcode'] == bc]                             # gather entries of sample with bc = bc\n",
    "    df_bc_unmapped = df_bc[df_bc['unmapped reads'] > 0]         # only include cluster with unmapped reads\n",
    "    df_bc_unmapped_list = df_bc_unmapped['cluster'].tolist()    # list those clusters with unmapped reads\n",
    "    for i, j in enumerate(df_bc_unmapped_list):\n",
    "        j = within_sampe_clustering_dir + '/' + bc + '/clusters_reads_to_consout_mapping/' + j    # complete path of .sam\n",
    "        df_bc_unmapped_list[i] = j                                                                # replace sam with pathed sam\n",
    "\n",
    "    # list all depth-filtered-clusters    \n",
    "    depth_filtered_clusters_args = within_sampe_clustering_dir + '/' + bc + '/depth_filtered_clusters/*_cluster_*'\n",
    "    depth_filtered_clusters = glob.glob(depth_filtered_clusters_args)\n",
    "    \n",
    "    \n",
    "    loopcounter2 = 0  \n",
    "    \n",
    "    # list all unmapped reads of cluster 'k'\n",
    "    for k in df_bc_unmapped_list:                        # loop through the sam files\n",
    "        unmapped_reads = []                              # placeholder list to append read ids of unmapped reads to\n",
    "        with open(k) as f:                               # open sam file\n",
    "            for line in f:                               # loop through sam file line per line\n",
    "                if not line.startswith('@'):             # ignore header\n",
    "                    line = line.split('\\t')              # split lines by tab-del\n",
    "                    if line[2] == '*':                   # gather unmapped entries\n",
    "                        unmapped_reads.append(line[0])   # and append to list of unmapped reads of current cluster\n",
    "            \n",
    "            \n",
    "        # navigate to cluster in depth_filtered_clusters, open cluster, list cluster, remove unmapped entries, write only mapped reads to new file in new directory\n",
    "        k = k.replace('.sam', '')                        # remove the .sam from the element, so the file name = cluster name\n",
    "\n",
    "        for cluster in depth_filtered_clusters:                            # loop through all depth-filtered clusters\n",
    "            if path_leaf(cluster) == path_leaf(k):                         # find the matching cluster by their names\n",
    "                \n",
    "                loopcounter2 = loopcounter2 + 1\n",
    "                \n",
    "                marked_unmapped_reads = []\n",
    "                with open(cluster) as f:                                   # open the cluster if file name matches\n",
    "\n",
    "                    \n",
    "                    # parse consout.fasta\n",
    "                    parsed_fasta = f.read().split('>')\n",
    "                    while '' in parsed_fasta:\n",
    "                        parsed_fasta.remove('')\n",
    "                    parsed_fasta = ['>' + l for l in parsed_fasta]\n",
    "\n",
    "                for entry in parsed_fasta:                                 # append all unmapped reads to a list\n",
    "                    for unmapped_read in unmapped_reads:\n",
    "                        if unmapped_read in entry:\n",
    "                            marked_unmapped_reads.append(entry)\n",
    "\n",
    "                mapped_reads_only = list(set(parsed_fasta)^set(marked_unmapped_reads))            # remove unmapped reads from the parsed cluster.fasta\n",
    "\n",
    "                mapped_reads_only_cluster = mapped_reads_only_dir + '/' + path_leaf(cluster)      # define path and name for new file with only mapped reads\n",
    "\n",
    "                with open(mapped_reads_only_cluster, 'x') as f:                                   # write only the mapped reads into new file\n",
    "                    for entry in mapped_reads_only:\n",
    "                        f.write(entry)\n",
    "                        \n",
    "                        \n",
    "                # move mapped reads only cluster to 'depth-filtered_clusters' and replace its old cluster with unmapped reads\n",
    "                new_file_path = within_sampe_clustering_dir + '/' + bc + '/depth_filtered_clusters/' + path_leaf(mapped_reads_only_cluster)\n",
    "                shutil.move(mapped_reads_only_cluster, new_file_path)\n",
    "\n",
    "                print('Sample', loopcounter, 'out of', len(barcodes), ': Filter out unmapped reads from', bc, 'clusters:', \n",
    "                      loopcounter2, 'out of', len(df_bc_unmapped_list), 'clusters with unmapped reads', end='\\r')\n",
    "     \n",
    "    \n",
    "# delete 'mapped_reads_only_dir'\n",
    "                                            \n",
    "print('\\n')\n",
    "\n",
    "# count reads per cluster, remove clusters with less than 10 mapped reads\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for bc in barcodes:\n",
    "    loopcounter = loopcounter + 1\n",
    "    print('Sample', loopcounter, 'out of', len(barcodes), ': Remove', bc, 'clusters with less than', minimum_depth, 'mapped reads', end='\\r')\n",
    "    \n",
    "    depth_filtered_clusters_args = within_sampe_clustering_dir + '/' + bc + '/' + 'depth_filtered_clusters/*_cluster_*'\n",
    "    depth_filtered_clusters = glob.glob(depth_filtered_clusters_args)\n",
    "\n",
    "    low_unmapped_count = []                           # placeholder list for paths to clusters with less than 'minimum_depth' mapped reads\n",
    "    \n",
    "    for i in depth_filtered_clusters:                 # loop through all clusters in '/depth_filtered_clusters/'\n",
    "        with open(i) as f:                            # open the cluster.fasta to count entries by counting '>'\n",
    "            n = 0\n",
    "            for line in f:\n",
    "                if line[0] == '>':\n",
    "                    n = n + 1\n",
    "        \n",
    "        if n < minimum_depth:\n",
    "            low_unmapped_count.append(i)             # append clusters with less than 'minimum_depth' mapped reads to placeholder list\n",
    "    \n",
    "    for i in low_unmapped_count:\n",
    "        os.remove(i)                                 # remove the clusters with less than 'minimum_depth' mapped reads\n",
    " \n",
    "print('\\n')\n",
    "\n",
    "# create the directory for among-samples clustering\n",
    "\n",
    "in_between_samples_clustering_dir = analysis_dir + '/among_samples_clustering'         # path the directory into the analysis directory\n",
    "\n",
    "print('Creating a working directory for the \"among-samples clustering\":', in_between_samples_clustering_dir, '\\n')\n",
    "\n",
    "if not os.path.exists(in_between_samples_clustering_dir):\n",
    "    os.makedirs(in_between_samples_clustering_dir)                                         # create the directory if it does not exist\n",
    "    \n",
    "    \n",
    "# list all consout\n",
    "\n",
    "list_of_filtered_consout = []                                                                                             # empty list to append to from loop below\n",
    "\n",
    "for i in range(len(barcodes)):                                                                                            # loop through barcodes\n",
    "    filtered_consout = within_sampe_clustering_dir + '/' + barcodes[i] + '/consout_' + barcodes[i] + '_filtered.fasta'    # get path of consout_*_filtered.fasta\n",
    "    list_of_filtered_consout.append(filtered_consout)                                                                     # append path of consout*filtered.fasta to list\n",
    "\n",
    "\n",
    "# cat all consout_*_filtered.fasta files\n",
    "\n",
    "cat_consout = in_between_samples_clustering_dir + '/cat_filtered_consout_wsc.fasta'\n",
    "\n",
    "if os.path.exists(cat_consout):\n",
    "    os.remove(cat_consout)\n",
    "\n",
    "print('Concatenate', len(barcodes), 'samples to', cat_consout, '\\n')\n",
    "    \n",
    "with open(cat_consout,'wb') as wfd:          # create and open in binary mode the cat_filtered_consout.fasta to write in\n",
    "    for f in list_of_filtered_consout:       # loop through the filtered consouts\n",
    "        with open(f,'rb') as fd:             # open a filtered consout\n",
    "            shutil.copyfileobj(fd, wfd)      # copy content of filtered consout to the cat_filtered_consout.fasta\n",
    "\n",
    "\n",
    "# create directory for among-samples cluster files            \n",
    "\n",
    "msaout = in_between_samples_clustering_dir + '/msaout'                                # define the vsearch msaout path and name\n",
    "consout = in_between_samples_clustering_dir + '/consout'                              # define the vsearch consout path and name\n",
    "clusters_dir = in_between_samples_clustering_dir + '/clusters'                        # define the vsearch clusters path and directory name\n",
    "\n",
    "if not os.path.exists(clusters_dir):                                                  # if the clusters directory does not yet exist\n",
    "    os.makedirs(clusters_dir)                                                         # create it\n",
    "\n",
    "clusters = clusters_dir + '/wsc_cluster_'                                             # define the vsearch clusters names\n",
    "stdout_path = in_between_samples_clustering_dir + '/vsearch_stdout'                   # define the vsearch stdout\n",
    "stderr_path = in_between_samples_clustering_dir + '/vsearch_stderr'                   # define the vsearch stderr\n",
    "\n",
    "vsearch_input = cat_consout                                                           # define the path to the correct sample\n",
    "\n",
    "vsearch_wsc_args = ['vsearch', \n",
    "                    '--cluster_fast', vsearch_input, \n",
    "                    '--id', in_between_samples_clustering_ct, \n",
    "                    '--msaout', msaout, \n",
    "                    '--consout', consout, \n",
    "                    '--clusters', clusters, \n",
    "                    '--threads', threads]                                             # list of the vsearch command line input formatted for the subprocess package\n",
    "\n",
    "if os.path.exists(stdout_path):                                                       # remove vsearch stdout and stderr if it already exists from previous runs\n",
    "    os.remove(stdout_path)\n",
    "if os.path.exists(stderr_path):\n",
    "    os.remove(stderr_path)\n",
    "\n",
    "print('among-samples clustering: clustering filtered within-sample consout of all samples')\n",
    "print('vsearch --id:', in_between_samples_clustering_ct)\n",
    "print('consout:', consout, '\\n')\n",
    "\n",
    "with open(stdout_path, 'x') as stdout_file:                                                                   # create and open files to write stderr and stdout to\n",
    "    with open(stderr_path, 'x') as stderr_file:\n",
    "        run_vsearch_wsc = subprocess.run(vsearch_wsc_args, stdout = stdout_file, stderr = stderr_file)        # run vsearch\n",
    "    \n",
    "        if run_vsearch_wsc.returncode == 0:                                                                   # print subprocess.run stdout if vsearch process was successful\n",
    "            print('Done. Details are written to', stderr_path, '\\n')\n",
    "            #print(run_vsearch_wsc, '\\n')\n",
    "            with open(stderr_path) as f:\n",
    "                print(f.read())\n",
    "        if run_vsearch_wsc.returncode == 1:                                                                   # print a warning message if vsearch process was unsuccessful\n",
    "            print('Warning. Something went wrong concerning the within-sample-clustering of', barcodes[i])\n",
    "            print(run_vsearch_wsc, '\\n')\n",
    "\n",
    "\n",
    "# count reads in every cluster = clustercount\n",
    "\n",
    "clusters_in_dir = clusters_dir + '/*'\n",
    "list_clusters = glob.glob(clusters_in_dir)                                       # list all clusters in /in_between_samples_clustering/clusters\n",
    "\n",
    "\n",
    "# create clustercount.csv\n",
    "\n",
    "clustercount_dir = in_between_samples_clustering_dir + '/clustercount.csv'       # name and path the clustercount.csv\n",
    "\n",
    "if os.path.exists(clustercount_dir):\n",
    "    os.remove(clustercount_dir)                                                  # remove clustercount.csv if it already exists\n",
    "if not os.path.exists(clustercount_dir):\n",
    "    with open(clustercount_dir, 'x') as f:\n",
    "        f.write('cluster\\tcount\\n')                                              # create and open clustercount.csv if it does not yet exist and add header columns 'cluster' and 'count'\n",
    "\n",
    "for i in list_clusters:                                                          # loop through the listed clusters\n",
    "    with open(i) as f:                                                           # open the cluster\n",
    "        n = 0\n",
    "        n_reads = []\n",
    "        for line in f:\n",
    "            if line[0] == '>':\n",
    "                n = n + 1                                                        # if a line starts with '>' (= fasta header), count + 1\n",
    "        n_reads.append(n)                                                        # append count to n_reads variable\n",
    "        \n",
    "        with open(clustercount_dir, 'a') as f:                                   # open the clustercount.csv for appending n_reads of cluster i\n",
    "            f.write(path_leaf(i))\n",
    "            f.write('\\t')\n",
    "            f.write(str(n_reads[0]))\n",
    "            f.write('\\n')\n",
    "\n",
    "print('Clustercount.csv created in', clustercount_dir, '\\n')\n",
    "\n",
    "df = pd.read_csv(clustercount_dir, sep='\\t')\n",
    "print('Total reads:\\t\\t\\t', df['count'].sum())\n",
    "print('Mean reads in cluster:\\t\\t', round(df['count'].mean(), 2), '+-', round(df['count'].std(), 2))\n",
    "print('Singleton clusters:\\t\\t', df['count'].isin([1]).sum())\n",
    "print('% Singleton clusters:\\t\\t', (round(df['count'].isin([1]).sum() / df['count'].sum(), 4) * 100))\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# create clustercount_hist.csv\n",
    "\n",
    "clustercount_df = pd.read_csv(clustercount_dir, sep='\\t')                        # open clustercount.csv in pandas\n",
    "clustercount_hist = clustercount_dir.replace('.csv', '_hist.csv')                # define clustercount_hist.csv path and name\n",
    "clustercount_df.groupby('count').count().to_csv(clustercount_hist, sep=\"\\t\")     # group clustercount.csv by column 'count' and output to clustercount_hist.csv\n",
    "\n",
    "print('Clustercount_hist.csv created in', clustercount_hist)\n",
    "\n",
    "\n",
    "# filter out loci cluster with multiple occurences of a sample\n",
    "\n",
    "df_filtered = df.loc[(df['count'] >= minimum_sample_per_locus) & (df['count'] <= len(barcodes))]        # create a dataframe filtering out below minimum sample per locus and above total\n",
    "                                                                                                        # amout of sample = len(barcodes)\n",
    "    \n",
    "df_filtered_list = df_filtered['count'].tolist()                                                        # puts the cluster names in df_filtered to a list\n",
    "\n",
    "# loop through list and count occurences of barcodes, if it is higher than 1, remove from list.\n",
    "# the remaining clusters are completely filtered, move on to read_aligner.ipynb\n",
    "\n",
    "print('\\n')\n",
    "print(\"Total clusters:\\t\\t\\t\", len(df.index))\n",
    "print(\"Singleton clusters:\\t\\t\", len(df.loc[(df['count'] < minimum_sample_per_locus)]))\n",
    "print(\"clusters >\", len(barcodes), 'samples:\\t\\t', len(df.loc[(df['count'] > len(barcodes))]))\n",
    "print(\"Potentially usable clusters:\\t\", len(df_filtered.index))  # clusters/loci left after initial filtering\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "df_filtered_list = df_filtered['cluster'].tolist()\n",
    "\n",
    "print('prefiltered loci:', len(df_filtered_list))\n",
    "\n",
    "\n",
    "# create a csv to log occurence of samples in each loci cluster\n",
    "\n",
    "loci_info_path = in_between_samples_clustering_dir + '/loci_info.csv'\n",
    "\n",
    "if os.path.exists(loci_info_path):      # remove loci_info.csv if it already exists (from previous runs)\n",
    "    os.remove(loci_info_path)\n",
    "\n",
    "with open(loci_info_path, 'x') as f:    # create the loci_info.csv and prepare the header\n",
    "    f.write('cluster\\t')\n",
    "    for i in barcodes:\n",
    "        f.write(i)\n",
    "        f.write('\\t')\n",
    "    f.write('\\n')\n",
    "\n",
    "    \n",
    "# count sample occurence and log to csv    \n",
    "    \n",
    "for i in df_filtered_list:\n",
    "    path_i = clusters_dir + '/' + i\n",
    "    #print(path_i)\n",
    "    with open(loci_info_path, 'a') as outfile:\n",
    "        outfile.write(i)\n",
    "        outfile.write('\\t')\n",
    "        with open(path_i) as f:\n",
    "            content = f.read()\n",
    "            for j in barcodes:\n",
    "                count_arg = '>' + j\n",
    "                count_barcodes = content.count(count_arg)\n",
    "                #print(j, count_barcodes)\n",
    "                outfile.write(str(count_barcodes))\n",
    "                outfile.write('\\t')\n",
    "        outfile.write('\\n')\n",
    "\n",
    "print('\\n')\n",
    "        \n",
    "    \n",
    "# loop through list and count occurences of barcodes, if it is higher than 1, remove from list.\n",
    "\n",
    "print('Now removing Loci with multiple occurence of a sample consout...')\n",
    "\n",
    "to_remove_list = []\n",
    "\n",
    "for i in df_filtered_list:\n",
    "    path_i = clusters_dir + '/' + i\n",
    "    #print(path_i)\n",
    "    with open(path_i) as f:\n",
    "        content = f.read()\n",
    "        for j in barcodes:\n",
    "            count_arg = '>' + j # specifies search string just in case the centroid read-id in the header randomly contains 'bcXX'\n",
    "            count_barcodes = content.count(count_arg) # used 'count_arg' instead of just 'j' for the reason explained above\n",
    "            #print(count_barcodes)\n",
    "            if count_barcodes >= 2:\n",
    "                print(i, 'is marked for removal due to multiple occurence of the same sample in loci', end='\\r')\n",
    "                to_remove_list.append(i)\n",
    "\n",
    "df_filtered_list = [x for x in df_filtered_list if x not in to_remove_list]            # removes the list entries of 'to_remove_list' in 'df_filtered_list'\n",
    "\n",
    "print('\\n')\n",
    "print('Loci remaining after filtering multiple sample occurences:', len(df_filtered_list))\n",
    "#print(df_filtered_list)\n",
    "\n",
    "\n",
    "# remove the tabstop at the end of lines\n",
    "\n",
    "df_loci_csv = pd.read_csv(loci_info_path, delimiter='\\t')\n",
    "df_loci_csv.drop(df_loci_csv.columns[len(df_loci_csv.columns)-1], axis=1, inplace=True)\n",
    "df_loci_csv.to_csv(loci_info_path, sep='\\t', index=False)\n",
    "\n",
    "print('\\nA table documenting occurence of samples per loci is placed to', loci_info_path)\n",
    "\n",
    "\n",
    "# use df_filtered_list to search the filtered clusters and move them to a new directory\n",
    "\n",
    "clusters_filtered_dir = in_between_samples_clustering_dir + '/clusters_filtered'\n",
    "\n",
    "if os.path.exists(clusters_filtered_dir):\n",
    "    shutil.rmtree(clusters_filtered_dir)\n",
    "if not os.path.exists(clusters_filtered_dir):\n",
    "    os.makedirs(clusters_filtered_dir)\n",
    "\n",
    "for i in df_filtered_list:\n",
    "    src = clusters_dir + '/' + i\n",
    "    dst = clusters_filtered_dir + '/' + i\n",
    "    shutil.move(src, dst)\n",
    "    #print(src, 'moved to', dst)\n",
    "    \n",
    "print('The filtered loci have been moved to', clusters_filtered_dir, '\\n')\n",
    "\n",
    "\n",
    "# Prepare 'infos' files, which contain sample/bc and its centroid read id, homologized by the among-samples clustering (= a locus)\n",
    "\n",
    "infos_dir = in_between_samples_clustering_dir + '/infos'         # name and path the infos file directory\n",
    "\n",
    "if os.path.exists(infos_dir):                                    # remove the infos file directory with content if it already exists\n",
    "    shutil.rmtree(infos_dir)\n",
    "if not os.path.exists(infos_dir):                                # make the infos file directory if it does not exist\n",
    "    os.makedirs(infos_dir)\n",
    "    \n",
    "filtered_clusters_list_arg = clusters_filtered_dir + '/*'\n",
    "filtered_clusters_list = glob.glob(filtered_clusters_list_arg)   # make a list with all filtered clusters\n",
    "\n",
    "for i in range(len(filtered_clusters_list)):\n",
    "    infos_dir_output = infos_dir + '/infos_' + path_leaf(filtered_clusters_list[i])\n",
    "    write_infos_to_file(get_infos_from_cluster_file(filtered_clusters_list[i]), infos_dir_output)      # write infos file for each filtered cluster\n",
    "\n",
    "info_file_list_arg = infos_dir + '/infos*'\n",
    "info_file_list = glob.glob(info_file_list_arg)                   # list all infos files\n",
    "\n",
    "list_of_dfs = [pd.read_csv(file, delimiter='\\t') for file in info_file_list]                           # iterate through the infos files and list them\n",
    "\n",
    "for dataframe, cluster_name in zip(list_of_dfs, info_file_list):                                       # add column 'cluster_name' to the info files\n",
    "    dataframe['cluster_name'] = path_leaf(cluster_name)\n",
    "\n",
    "\n",
    "# list all filtered nanopore reads.fastq\n",
    "\n",
    "filtered_reads_arg = filtered_reads_dir + '*.fastq'\n",
    "filtered_reads_list = glob.glob(filtered_reads_arg)\n",
    "\n",
    "\n",
    "# create new directory for the filtered out fastq reads (done below), if it does not exist\n",
    "\n",
    "filtered_fastq_dir = in_between_samples_clustering_dir  + '/filtered_fastq'      # name the directory for the fastq files with only the content of the cluster\n",
    "\n",
    "if os.path.exists(filtered_fastq_dir):                                           # check if directory exists\n",
    "    shutil.rmtree(filtered_fastq_dir)                                            # remove directory with content\n",
    "if not os.path.exists(filtered_fastq_dir):                                       # if directory does not exist\n",
    "    os.makedirs(filtered_fastq_dir)                                              # make the directory\n",
    "    print('Filtered reads directory:', filtered_fastq_dir)\n",
    "\n",
    "\n",
    "# create a file to log unmapped reads\n",
    "\n",
    "unmapped_reads_log_dir = in_between_samples_clustering_dir + '/unmapped_reads_log.csv'\n",
    "\n",
    "if os.path.exists(unmapped_reads_log_dir):                                                          # check if missing reads in the mapping file exists\n",
    "    os.remove(unmapped_reads_log_dir)                                                               # remove it\n",
    "if not os.path.exists(unmapped_reads_log_dir):                                                      # if file does not exist\n",
    "    with open(unmapped_reads_log_dir, 'x') as f:                                                    # make it\n",
    "        f.write('locus\\tcluster\\tunmapped reads\\ttotal reads in cluster\\tPercentage unmapped reads\\n')     # and write the header\n",
    "    print('Unmapped reads are logged to:', unmapped_reads_log_dir)\n",
    "    \n",
    "\n",
    "# create a dictionary of all consout consensus sequences generated after the second (among samples) clustering\n",
    "\n",
    "consout_infile = consout\n",
    "consout_dict = consout_to_dict(consout_infile)\n",
    "print(\"Total count of second (among-samples) clustering consout consensus sequences:\", len(consout_dict), \"\\n\")\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in range(len(list_of_dfs)):\n",
    "    loopcounter = loopcounter + 1\n",
    "    print('infos_cluster number', loopcounter, 'out of', len(info_file_list), '- Currently iterating through', list_of_dfs[i]['cluster_name'][0], '...', end='\\r') \n",
    "    \n",
    "    # create seperate directory for the outputs of each info_file\n",
    "    \n",
    "    info_output_dir = filtered_fastq_dir + '/output_' + list_of_dfs[i][\"cluster_name\"][0]\n",
    "    if os.path.exists(info_output_dir):\n",
    "        shutil.rmtree(info_output_dir)\n",
    "    if not os.path.exists(info_output_dir):\n",
    "        os.makedirs(info_output_dir)\n",
    "    \n",
    "    \n",
    "    # write the corresponding consout consensus sequence of the in-between-samples clustering as a fasta reference to 'info_output_dir'\n",
    "    \n",
    "    corresponding_consout_name = info_output_dir + '/' + list_of_dfs[i]['cluster_name'][0] + '_reference.fasta'\n",
    "    fasta_reference_formatting = '>' + list_of_dfs[i]['cluster_name'][0] + '_reference\\n' + consout_dict[list_of_dfs[i]['read_id'][0]]\n",
    "    \n",
    "    with open(corresponding_consout_name, 'x') as f:\n",
    "        f.write(fasta_reference_formatting)\n",
    "\n",
    "        \n",
    "        \n",
    "    # loop through all loci to identify fastq reads of each sample to map against the pseudoreference generated by within-samples clustering vsearch --consout\n",
    "        \n",
    "    for j in range(len(list_of_dfs[i])):                                                 # loop through lines (=samples) in the 'list_of_dfs'\n",
    "        #print(j)\n",
    "        for k in barcodes:                                                               # loop through the barcodes\n",
    "            if list_of_dfs[i]['barcode'][j] == k:                                        # if a barcode matches the barcode of current line (=sample):\n",
    "                #print(list_of_dfs[i]['barcode'][j], 'is', k)\n",
    "                path_to_fastq_arg = filtered_reads_dir + k + '*.fastq'               \n",
    "                path_to_fastq = glob.glob(path_to_fastq_arg)                             # choose the bcXX*.fastq Nanopore filtered read \n",
    "                if len(path_to_fastq) >= 2:\n",
    "                    print('More than 1 FASTQ-file was found for sample', k, '. Please make sure to remove similar named .fastq files with the same barcode/identifier from the directory')\n",
    "                    break                                                                # Stop the loop if somehow 2 files with the same barcode were chosen as 'path_to_fastq'\n",
    "                if len(path_to_fastq) == 0:\n",
    "                    print('No filtered reads.fastq could be identified. Make sure to place the exact barcode affilitation into the file name, e.g. \"bc01.fastq\"')\n",
    "                    break                                                                # Stop the loop if no reads matching the barcode were identified\n",
    "                if len(path_to_fastq) == 1:                                              # but continue if there is a single match for Nanopore filtered fastqs in 'path_to_fastq'\n",
    "                    #print(k, ':', path_leaf(path_to_fastq[0]).replace('_q7_200to1000bp.fastq', ''))\n",
    "                    wsc_clusters_arg = within_sampe_clustering_dir + '/' + list_of_dfs[i]['barcode'][j] + '/depth_filtered_clusters/*_cluster_*'\n",
    "                    wsc_clusters = glob.glob(wsc_clusters_arg)                           # list all within-samples clusters of the specified sample\n",
    "                    #print('Extracting', list_of_dfs[i]['barcode'][j], 'reads:')\n",
    "                    \n",
    "                    for cluster in wsc_clusters:                                 # iterates through clusters(.fasta) in path chosen in 'wsc_clusters'\n",
    "                        with open(cluster, 'r') as f:\n",
    "                            if list_of_dfs[i]['read_id'][j] in f.read():         # if the sample's centromer read_id matches the read_id in a cluster:\n",
    "                                #print('Found reads in:', cluster)\n",
    "                                \n",
    "                                with open(cluster, 'r') as f:                    # open and work with this cluster\n",
    "                                    \n",
    "                                    # parse the cluster.fasta and place read ids in a list\n",
    "                                    content = f.read().split('>')             # list the content by splitting the fasta by '>'\n",
    "                                    while '' in content:\n",
    "                                        content.remove('')                       # removes empty list contents (the first element is always empty)\n",
    "                                    read_ids = []                                # create list to append read ids of current cluster to\n",
    "                                    for element in content:                      # loop through each fasta entry in the cluster.fasta\n",
    "                                        header = element[0:element.find('\\n')]   # takes the header = Nanopore read id \n",
    "                                        read_ids.append(header)                  # append the header/read id to the read_ids list\n",
    "                                    #print('Read ids in cluster:', len(read_ids))\n",
    "\n",
    "                                    \n",
    "                                    # parse the filtered reads.fastq for the current sample\n",
    "            \n",
    "                                    fastq_reads = []                             # an empty list to append fastq reads to (see below)\n",
    "\n",
    "                                    with open(path_to_fastq[0]) as fq_f:         # open the filtered reads.fastq of the current sample\n",
    "                                        for entry in take_four(fq_f):\n",
    "                                            fastq_reads.append(''.join(entry))   # append each fastq entry as a single element to list 'fastq_reads'\n",
    "                                    \n",
    "                                    named_filtered_fastq = info_output_dir + '/' + list_of_dfs[i]['barcode'][j]\n",
    "                                    \n",
    "                                    with open(named_filtered_fastq, 'x') as f:\n",
    "                                        for fastq_entry in fastq_reads:\n",
    "                                            for read_id in read_ids:\n",
    "                                                if read_id in fastq_entry:\n",
    "                                                    f.write(fastq_entry)\n",
    "                                        \n",
    "                                        #print('Done writing the fastq of', cluster)\n",
    "                                    \n",
    "                                    \n",
    "                                    # mapping reads extracted just now to pseudoreference (within-samples clustering consout)\n",
    "                                    \n",
    "                                    corresponding_consout_name   # reference\n",
    "                                    named_filtered_fastq         # input fastq reads\n",
    "                                    \n",
    "                                    stderr_path = named_filtered_fastq + '_stderr'     # stderr output from mapping = minimap2 printout\n",
    "                                    stdout_path = named_filtered_fastq + '.sam'        # mapping output .sam file\n",
    "                                    \n",
    "                                    if os.path.exists(stderr_path):    # remove mapping outputs from previous runs\n",
    "                                        os.remove(stderr_path)\n",
    "                                    if os.path.exists(stdout_path):\n",
    "                                        os.remove(stdout_path)\n",
    "                                    \n",
    "                                    with open(stderr_path, 'x') as stderr_outfile:\n",
    "                                        with open(stdout_path, 'x') as stdout_outfile:\n",
    "                                            mapping = subprocess.run(['minimap2', \n",
    "                                                                      '-ax', \n",
    "                                                                      'map-ont', \n",
    "                                                                      '-t', threads, \n",
    "                                                                      corresponding_consout_name,  # reference path\n",
    "                                                                      named_filtered_fastq],       # input fastq path\n",
    "                                                                      stderr = stderr_outfile,     # minimap2 printout\n",
    "                                                                      stdout = stdout_outfile      # minimap2 .sam output\n",
    "                                                                    )\n",
    "                                    \n",
    "                                    if mapping.returncode == 1:\n",
    "                                        print('Error. Subprocess for minimap2 mapping returncode = 1. Please check your output.')\n",
    "                                        break\n",
    "                                    #if mapping.returncode == 0:\n",
    "                                    #    print('Successfully mapped', path_leaf(named_filtered_fastq), 'to', path_leaf(corresponding_consout_name))\n",
    "                                        \n",
    "                                    \n",
    "                                    # counting unmapped reads per sample by comparing n-2 lines of the .sam file ('stdout_path') to total reads ('read_ids')\n",
    "                                    # but first, remove unmapped reads and supplemental alignments (duplicated reads) from the minimap2 .sam output\n",
    "                                    \n",
    "                                    filtered_sam = stdout_path.replace('.sam', '_filtered.sam')\n",
    "                                    filter_sam_process = subprocess.run(['samtools', 'view', \n",
    "                                                                                     '-hF', '2052', \n",
    "                                                                                     '-o', filtered_sam, \n",
    "                                                                                     stdout_path])\n",
    "                                    \n",
    "                                    if filter_sam_process.returncode == 1:\n",
    "                                        print('Error. Subprocess for samtools view to remove supplemental alignments and unmapped reads: Returncode = 1')\n",
    "                                        break\n",
    "                                    #if remove_unmapped_reads_from_sam.returncode == 0:    # if samtools view worked, replace original .sam with new .sam\n",
    "                                    #    os.remove(stdout_path)\n",
    "                                    #    os.rename(no_unmapped_reads_sam, stdout_path)\n",
    "                                    \n",
    "                                    with open(filtered_sam) as sam_f:\n",
    "                                        num_lines = sum(1 for line in sam_f)\n",
    "                                        mapped_reads = num_lines - 2\n",
    "                                        \n",
    "                                    with open(unmapped_reads_log_dir, 'a') as unmapped_reads_f:\n",
    "                                        unmapped_reads_f.write(list_of_dfs[i]['cluster_name'][0])\n",
    "                                        unmapped_reads_f.write('\\t')\n",
    "                                        unmapped_reads_f.write(path_leaf(cluster))\n",
    "                                        unmapped_reads_f.write('\\t')\n",
    "                                        unmapped_reads_f.write(str(len(read_ids) - mapped_reads))\n",
    "                                        unmapped_reads_f.write('\\t')\n",
    "                                        unmapped_reads_f.write(str(len(read_ids)))\n",
    "                                        unmapped_reads_f.write('\\t')\n",
    "                                        unmapped_reads_f.write(str((len(read_ids) - mapped_reads) / len(read_ids)))\n",
    "                                        unmapped_reads_f.write('\\n')        \n",
    "                    \n",
    "    #print('\\nDone with', list_of_dfs[i]['cluster_name'][0], '\\n-------------------------------------------------------------------------------------------------------------------------\\n')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# unmapped reads stats\n",
    "\n",
    "df = pd.read_csv(unmapped_reads_log_dir, delimiter='\\t')\n",
    "print('\\nUnmapped reads:\\t\\t\\t\\t\\t', df['unmapped reads'].sum(), 'out of', df['total reads in cluster'].sum(), \n",
    "      '(', round((df['unmapped reads'].sum() / df['total reads in cluster'].sum()) * 100, ndigits=2), '% )')\n",
    "print('Clusters with unmapped reads:\\t\\t\\t', len(df[(df['unmapped reads'] >= 1)]), 'out of', len(df), \n",
    "      '(', round((len(df[(df['unmapped reads'] >= 1)]) / len(df)) * 100, ndigits=2), '% )')\n",
    "print('Sum of clusters with', minimum_depth, 'or more unmapped reads:\\t', len(df[(df['unmapped reads'] >= minimum_depth)]), \n",
    "      '(', round((len(df[(df['unmapped reads'] >= minimum_depth)]) / len(df)) * 100, ndigits=2))\n",
    "print('% clusters with', minimum_depth, 'or more unmapped reads:\\t', round(len(df[(df['unmapped reads'] >= minimum_depth)]) / len(df), ndigits=2))\n",
    "print('A detailed overview can be found in', unmapped_reads_log_dir)\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# sam to bam conversion, bam sorting and bam indexing\n",
    "\n",
    "sam_files_list_arg = filtered_fastq_dir + '/*/*_filtered.sam'\n",
    "sam_files_list = glob.glob(sam_files_list_arg)\n",
    "\n",
    "print('.sam to .bam conversion, bam sorting and indexing of')\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "for i in sam_files_list:\n",
    "    \n",
    "    loopcounter = loopcounter + 1\n",
    "    print(loopcounter, 'out of', len(sam_files_list), ':', i, end='\\r')\n",
    "    \n",
    "    bam_output = i.replace('_filtered.sam', '.bam')\n",
    "    sam_to_bam = subprocess.run(['samtools', 'view', '-b', '-F', '12', '-o', bam_output, i])\n",
    "    if sam_to_bam.returncode == 1:\n",
    "        print('something went wrong: sam to bam')\n",
    "        break\n",
    "    \n",
    "    sorted_bam_output = bam_output.replace('.bam', '_sorted.bam')\n",
    "    sort_bam = subprocess.run(['samtools', 'sort', '-o', sorted_bam_output, bam_output])                      # run samtools sort -o [out] [in]\n",
    "    if sort_bam.returncode == 1:\n",
    "        print('something went wrong: bam sorting')\n",
    "        break\n",
    "    \n",
    "    bai_output = sorted_bam_output.replace('.bam', '.bai')                                                    # the .bai output should habe the same name and directory as the .bam\n",
    "    index_bam = subprocess.run(['samtools', 'index', '-b', sorted_bam_output, bai_output])                    # run samtools index -b [in] [out]\n",
    "    if index_bam.returncode == 1:\n",
    "        print('something went wrong: bam indexing')\n",
    "        break    \n",
    "    \n",
    "    os.remove(bam_output)                                                                                     # removes the unsorted bam file\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "\n",
    "# collecting SNPs loci per loci with bcftools\n",
    "\n",
    "# create a log file counting the amount of SNPs called for each locus\n",
    "snp_log_file_dir = in_between_samples_clustering_dir +  '/snps_log.csv'  # name and path for the log file\n",
    "if os.path.exists(snp_log_file_dir):                                                                 # check if log file exists\n",
    "    os.remove(snp_log_file_dir)                                                                      # yes: remove it\n",
    "if not os.path.exists(snp_log_file_dir):                                                             # if file does not exist:\n",
    "    with open(snp_log_file_dir, 'x') as snp_log_file:                                                # creates the log file\n",
    "        snp_log_file.write('locus')                                                                  # write a header for tab-delimited text file\n",
    "        snp_log_file.write('\\t')\n",
    "        snp_log_file.write('SNPs')\n",
    "        snp_log_file.write('\\n')\n",
    "\n",
    "loci_list_arg = filtered_fastq_dir + '/*'\n",
    "list_of_all_loci = glob.glob(loci_list_arg)                      # collect all loci directories in order to loop through them\n",
    "\n",
    "loopcounter = 0\n",
    "\n",
    "print('Started calling SNPs for each loci:')\n",
    "\n",
    "for i in range(len(list_of_all_loci)):                                                          # loop through length of loci directories (= list_of_all_loci)\n",
    "\n",
    "    loopcounter = loopcounter + 1                                                               # counts + 1 every time the loop is started again\n",
    "    print(loopcounter, \"out of\", len(list_of_all_loci), ':', list_of_all_loci[i], end='\\r')     # shows progress of this loop\n",
    "    \n",
    "    \n",
    "    '''bcftools mpileup'''\n",
    "    '''Collects all possible variants found in the mapping against the (pseudo)reference = consout from vsearch 2nd (among-samples) clustering'''\n",
    "    \n",
    "    bam_in_dir = glob.glob(list_of_all_loci[i] + '/*_sorted.bam')                               # list filtered and sorted .bam files in current loci directory (= list_of_all_loci[i])\n",
    "    vcf_calling_ref = glob.glob(list_of_all_loci[i] + '/*_reference.fasta')                     # list the (pseudo)reference (vsearch consout of 2nd among-samples clustering) \n",
    "                                                                                                # in current loci directory (= list_of_all_loci[i])\n",
    "    \n",
    "    if not len(vcf_calling_ref) == 1:\n",
    "        print('More than one possible reference.fasta was found in', list_of_all_loci[i], 'make sure there is only one file called \"_reference.fasta\".')\n",
    "        break\n",
    "    \n",
    "    mpileup_out = vcf_calling_ref[0].replace('_reference.fasta', '_mpileup.vcf')                # correctly names the bcftools mpileup file + path\n",
    "    \n",
    "    # run bcftools mpileup\n",
    "    bcftools_mpileup_args = ['bcftools', 'mpileup', \n",
    "                             '--skip-indels', \n",
    "                             '--count-orphans', \n",
    "                             '--no-BAQ', \n",
    "                             '--min-BQ', '0', \n",
    "                             '-Ov', \n",
    "                             '--annotate', 'DP,AD,DP4', \n",
    "                             '-f', vcf_calling_ref[0], \n",
    "                             '-o', mpileup_out] + bam_in_dir\n",
    "    run_bcftools_mpileup = subprocess.run(bcftools_mpileup_args)\n",
    "    \n",
    "    if run_bcftools_mpileup.returncode == 1:                                                    # print a warning message in case there is an error creating the mpileup vcf file\n",
    "        print('Error. bcftools mpileup: returncode =/= 0!\\t')\n",
    "        break\n",
    "\n",
    "        \n",
    "    '''Change sample name'''\n",
    "    '''Samples in the mpileuxp.vcf are now named after the path of the input _filtered_sorted.bam file. This will cause problems in subsequent sample merging. Names will therefore\n",
    "    be changed to simply the barcode number.'''\n",
    "    \n",
    "    mpileup_edit_out = mpileup_out.replace('_mpileup.vcf', '_mpileup_edit.vcf')                    # defines the output path/name of the edited mpileup file with correct sample names\n",
    "       \n",
    "    with open(mpileup_out) as f:                                                                   # opens the *_mpileup.vcf file\n",
    "        correct_sample_names = f.read()                                                            # loads the text in *_mpileup.vcf\n",
    "        for j, k in enumerate(bam_in_dir):                                                         # loops through .bam files in 'current_locus' list\n",
    "            if bam_in_dir[j] not in correct_sample_names:                                          # print a warning message that sample in current_locus[j] was not found in mpileup.vcf\n",
    "                print('COULD NOT FIND', bam_in_dir[j])\n",
    "            else:\n",
    "                correct_sample_names = correct_sample_names.replace(bam_in_dir[j], path_leaf(k))   # if sample name was found in *_mpileup.vcf, replace path name for just the file name\n",
    "        correct_sample_names = correct_sample_names.replace('_sorted.bam', '')            # previous task leaves *_filtered_sorted.bam in sample name, remove this so only\n",
    "                                                                                                   # the barcode remains as the sample name (e.g. bc01 instead of bc01_filtered_sorted.bam)\n",
    "        if os.path.exists(mpileup_edit_out):\n",
    "            os.remove(mpileup_edit_out)                                                            # remove the *_mpileup_edit.vcf file if it already exists (from a previous run)\n",
    "\n",
    "    with open(mpileup_edit_out, 'x') as samples_named_vcf:                                         # create and open the *mpileup_edit.vcf file\n",
    "        samples_named_vcf.write(correct_sample_names)                                              # write the corrected sample names in *_mpileup_edit.vcf\n",
    "    \n",
    "    \n",
    "    '''bcftools norm -m -'''\n",
    "    '''Splits multiallelic sites so every variant gets its own vcf entry. This makes it easier to filter for true variants in the next steps'''\n",
    "    \n",
    "    norm_split_out = mpileup_out.replace('_mpileup.vcf', '_norm_split.vcf')                     # correctly names the bcftools norm -m - output file and path\n",
    "    \n",
    "    # run bcftools norm -m -\n",
    "    bcftools_norm_split_args = ['bcftools', 'norm', '-m', '-', '-Ov', '-o', norm_split_out, mpileup_edit_out]\n",
    "    run_bcftools_norm_split = subprocess.run(bcftools_norm_split_args)\n",
    "    \n",
    "    if run_bcftools_norm_split.returncode == 1:                                                 # print a warning message in case there is an error creating the norm -m - vcf file\n",
    "        print('Error. bcftools norm -m -: returncode =/= 0!\\t')\n",
    "        break\n",
    "    \n",
    "    \n",
    "    '''bcftools view -i (AD[*:1]/FORMAT/DP) > 0.25 & MIN(FORMAT/DP) >= 5'''\n",
    "    '''Filters for true variants. Variants rarer than 25% and at positions with less than 5 coverage will be filtered out. Therefore SNPs must appear at least twice at minimum coverage. \n",
    "    Only applicable for diploid samples'''\n",
    "    \n",
    "    view_out = norm_split_out.replace('_norm_split.vcf', '_view.vcf')                           # correctly names the bcftools view output file and path\n",
    "    \n",
    "    # run bcftools view -i \"(AD[*:1]/FORMAT/DP) > 0.25 & MIN(FORMAT/DP) >= 5\"\n",
    "    bcftools_view_args = ['bcftools', 'view', '-i', '(AD[*:1]/FORMAT/DP) > 0.25 & MIN(FORMAT/DP) >= 5', '-o', view_out, norm_split_out]\n",
    "    run_bcftools_view = subprocess.run(bcftools_view_args)\n",
    "\n",
    "    if run_bcftools_view.returncode == 1:                                                       # print a warning message in case there is an error creating the view vcf file\n",
    "        print('Error. bcftools view: returncode =/= 0!\\t')\n",
    "        break\n",
    "\n",
    "        \n",
    "    '''bcftools norm -m +'''\n",
    "    '''Combines multiallelic sites, so every site/position will be a single entry in the finalized vcf for the locus'''\n",
    "    \n",
    "    locus_vcf_out = view_out.replace('_view.vcf', '_locus.vcf')                                 # correctly names the bcftools norm -m + output file and path\n",
    "    \n",
    "    # run bcftools norm -m +\n",
    "    bcftools_norm_combine_args = ['bcftools', 'norm', '-m', '+', '-Ov', '-o', locus_vcf_out, view_out]\n",
    "    run_bcftools_norm_combine = subprocess.run(bcftools_norm_combine_args)\n",
    "    \n",
    "    if run_bcftools_norm_combine.returncode == 1:                                               # print a warning message in case there is an error creating the view vcf file\n",
    "        print('Error. bcftools norm -m +: returncode =/= 0!\\t')\n",
    "        break\n",
    "\n",
    "        \n",
    "    # count amount of SNPs of current locus and append to the log file\n",
    "    linecount = 0\n",
    "    with open(locus_vcf_out) as f:\n",
    "        for line in f:\n",
    "            if not line.startswith('#'):\n",
    "                linecount += 1\n",
    "                \n",
    "        with open(snp_log_file_dir, 'a') as snp_log_file:\n",
    "            snp_log_file.write(path_leaf(list_of_all_loci[i]))\n",
    "            snp_log_file.write('\\t')\n",
    "            snp_log_file.write(str(linecount))\n",
    "            snp_log_file.write('\\n')           \n",
    "\n",
    "            \n",
    "# printed outputs\n",
    "\n",
    "print('\\r\\nDone with SNP calling.\\n')\n",
    "snps_df = pd.read_csv(snp_log_file_dir, sep='\\t')                                      # opens the snp_log file for statistics\n",
    "print('Total amount of SNPs:', snps_df['SNPs'].sum())                                  # prints total SNPs\n",
    "print('Mean SNPs per locus:', snps_df['SNPs'].mean(), '+/-', snps_df['SNPs'].std())    # prints mean SNPs per locus + standard deviation\n",
    "print('Median SNPs per locus:', snps_df['SNPs'].median())                              # prints median SNPs per locus\n",
    "print('Loci with 0 SNPs:', snps_df['SNPs'].isin([0]).sum(), '\\n')                      # prints amount of loci with 0 SNPs\n",
    "\n",
    "\n",
    "list_of_all_vcf_arg = filtered_fastq_dir + '/*/*_locus.vcf'\n",
    "list_of_all_vcf = glob.glob(list_of_all_vcf_arg)                                    # lists all locus VCF files\n",
    "vcfcombine_out = in_between_samples_clustering_dir + '/Analysis_SNPS_dotted.vcf'    # output path\n",
    "\n",
    "\n",
    "# building the subprocess.run *popenargs list with the command line for vcflib's vcfcombine \n",
    "\n",
    "vcfcombine_args = ['vcfcombine']            # start with the vcfcombine module\n",
    "vcfcombine_args.extend(list_of_all_vcf)     # add the list with the paths for all locus VCF files\n",
    "\n",
    "if os.path.exists(vcfcombine_out):          # delete the merged VCF file if it already exists from a previous run\n",
    "    os.remove(vcfcombine_out)\n",
    "\n",
    "with open(vcfcombine_out, 'x') as vcfcombine_stdout:                                  # create and open the file for the merged VCF output\n",
    "    run_vcfcombine = subprocess.run(vcfcombine_args, stdout = vcfcombine_stdout)      # runs vcfcombine and outputs in stdout = file made above with path from vcfcombine_out\n",
    "\n",
    "if run_vcfcombine.returncode == 1:                                                                                                # if the subprocess failed:\n",
    "    print('Error. vcfcombine returncode =/= 0!\\t')                                                       # print a warning message\n",
    "else:                                                                                                                             # if everything worked:\n",
    "    print('All', len(list_of_all_vcf), 'loci VCF files have been merged to a single one:', vcfcombine_out)    # print some infos about the run\n",
    "\n",
    "      \n",
    "# replace '.' in VCF for 0,0,0:0:0,0,0,0:0,0\n",
    "\n",
    "no_dot_vcf_outfile = vcfcombine_out.replace('Analysis_SNPS_dotted.vcf', 'analysis_SNPs.vcf')\n",
    "\n",
    "c = 0\n",
    "\n",
    "with open(no_dot_vcf_outfile, 'w') as of:\n",
    "    with open(vcfcombine_out) as f:\n",
    "        for line in f:\n",
    "            if line[0] == '#':\n",
    "                of.write(line)\n",
    "                #print(line)\n",
    "            else:\n",
    "                cols = line.split('\\t')\n",
    "                cols[-1] = cols[-1].rstrip()\n",
    "                for i in range(9, len(cols)):\n",
    "                    if cols[i][0] == '.':\n",
    "                        cols[i] = '0,0,0:0:0,0,0,0:0,0'\n",
    "                of.write('\\t'.join(cols) + '\\n')\n",
    "                \n",
    "print('\\nFinished at:', datetime.now().strftime('%d/%m/%Y %H:%M:%s'))\n",
    "print('Duration: %s minutes' % round(((time.time() - start_time) / 60), ndigits=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
